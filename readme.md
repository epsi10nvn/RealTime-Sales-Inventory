# Real-Time Retail Data Warehouse

## ğŸ“Œ Introduction

This project simulates a **data warehouse** system for a retail store, processing **real-time** purchase data from Kafka and storing it in PostgreSQL. The system supports sales data analysis, inventory control, and visual reporting.

## ğŸš€ Features

- **Simulates product import and sales operations** of the store.
- **Processes streaming data** from Kafka using Apache Spark.
- **Stores purchase data** in a **data warehouse** following the **star schema** model.
- **Monitors inventory levels** to prevent overselling.
- **Analyzes revenue & sales quantity**, displaying reports with **charts and graphs**.

## ğŸ›ï¸ Data Warehouse Architecture

The system uses a **star schema** with the following tables:

- **Fact Table:** `sales_fact` (contains purchase transaction data)
- **Dimension Tables:**
  - `product_dim` (product information)
  - `customer_dim` (customer information)
  - `time_dim` (purchase time)

## ğŸ“‚ Folder Structure

```
â”œâ”€â”€ analysis                        # Data analysis and visualization scripts
â”œâ”€â”€ config                          # Configuration files
â”‚   â”œâ”€â”€ postgres_setup.py           # PostgreSQL setup script
â”œâ”€â”€ input                           # Input data sources
â”‚   â”œâ”€â”€ backup                      # Backup data
â”‚   â”‚   â”œâ”€â”€ import                  # Backup of import data
â”‚   â”‚   â”œâ”€â”€ purchase                # Backup of purchase data
â”‚   â”œâ”€â”€ filestreaming_import_goods  # Input for import streaming
â”‚   â”œâ”€â”€ kafka_purchase              # Input for Kafka streaming purchases
â”œâ”€â”€ src                             # Source code
â”‚   â”œâ”€â”€ generate_simulation_data    # Simulated data generators
â”‚   â”œâ”€â”€ utils                       # Utility functions
â”‚   â”‚   â”œâ”€â”€ sent_json_to_kafka.py   # Send JSON data to Kafka
â”‚   â”œâ”€â”€ query_helper                # Helper functions for queries
â”‚   â”œâ”€â”€ spark_filestreaming_import_goods
â”‚   â”‚   â”œâ”€â”€ update_database_streaming.py # Updates inventory
â”‚   â”œâ”€â”€ spark_kafka_streaming_purchase
â”‚   â”‚   â”œâ”€â”€ handle_kafka_data.py    # Processes Kafka purchase data in real-time
```

## âš™ï¸ Technologies

- **Apache Spark**: Processes streaming data from Kafka.
- **PostgreSQL**: Stores data using the Data Warehouse model.
- **Python (pandas, matplotlib)**: For data analysis and chart plotting.
- **Kafka**: Receives purchase transaction data.

## ğŸ“Š Visualizations

The project provides examples of reports generated after 3 product imports and 50 purchase transactions:

- **Revenue by Product** (`pie chart`)
- **Quantity Sold per Produc** (`bar chart`)
- **Revenue per Customer** (`pie chart`)
- ...

Example: Revenue per Customer.

![example_figure](./images/Figure_5.png)

## ğŸ“– Usage

1. **Clone repo:**
   ```sh
   git clone https://github.com/epsi10nvn/RealTime-Sales-Inventory.git
   cd RealTime-Sales-Inventory
   ```
2. **Set up the environment:**
   - Install PostgreSQL, Kafka, and Spark.
   - Install required libs from `requirements.txt`.
3. **Running**
   Create tables in the PostgreSQL database
   ```sh
   python config/postgres_setup.py
   ```
   Generate import data
   ```sh
   python src/generate_simulation_data/generate_import_goods_invoices.py
   ```
   Process import data
   ```sh
   python scr/spark_filestreaming_import_goods/update_database_streaming.py
   ```
   Start the purchase order processing program
   ```sh
   python src/spark_kafka_streaming_purchase/handle_kafka_data.py
   ```
   Generate purchase transaction data
   ```sh
   python src/generate_simulation_data/generate_purchase_invoices.py
   ```
4. **Analyze the data**
   ```sh
   python analysis/analyze.py
   ```

## ğŸ’¡ Contributions

All contributions are welcome! Feel free to create a **pull request** or open an **issue** if you have any improvement ideas.
